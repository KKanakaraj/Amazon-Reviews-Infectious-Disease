{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import gzip\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEWS_PATH = \"reviews_Grocery_and_Gourmet_Food.json.gz\"\n",
    "META_PATH = \"meta_Grocery_and_Gourmet_Food.json.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sanitizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(path, outpath):\n",
    "    \"\"\"Converts a given compressed json to strict json and writes it in a new file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "    The file location of the gzip-compressed json file\n",
    "    outpath : str\n",
    "    The path to the desired output file location \n",
    "\n",
    "    \"\"\"\n",
    "    g = gzip.open(path, 'r')\n",
    "\n",
    "    out = open(outpath, 'w')\n",
    "\n",
    "    for l in g:\n",
    "        out.write(json.dumps(eval(l)) + '\\n')\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_OUTPATH = \"cleaned_meta.json\"\n",
    "REVIEWS_OUTPATH = \"cleaned_reviews.json\"\n",
    "\n",
    "sanitize(META_PATH, META_OUTPATH)\n",
    "sanitize(REVIEWS_PATH, REVIEWS_OUTPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEWS_PATH = \"cleaned_reviews.json\"\n",
    "META_PATH = \"cleaned_meta.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1ZQZ8RJS1XVTX</td>\n",
       "      <td>0657745316</td>\n",
       "      <td>gsxrgirl</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>No sugar, no GMO garbage, no fillers that come...</td>\n",
       "      <td>5</td>\n",
       "      <td>Best vanilla I've ever had</td>\n",
       "      <td>1381449600</td>\n",
       "      <td>10 11, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A31W38VGZAUUM4</td>\n",
       "      <td>0700026444</td>\n",
       "      <td>FIFA Lvr</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This is my absolute, undisputed favorite tea r...</td>\n",
       "      <td>5</td>\n",
       "      <td>Terrific Tea!</td>\n",
       "      <td>1354752000</td>\n",
       "      <td>12 6, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>Alicia b</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>grrrrrrr</td>\n",
       "      <td>1385942400</td>\n",
       "      <td>12 2, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QAAOLIXKV383</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>Danny K. Tilley \"Dan Tilley\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>The cart is fine and works for the purpose for...</td>\n",
       "      <td>3</td>\n",
       "      <td>Storage on Wheels Cart</td>\n",
       "      <td>1307836800</td>\n",
       "      <td>06 12, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AB1A5EGHHVA9M</td>\n",
       "      <td>141278509X</td>\n",
       "      <td>CHelmic</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This product by Archer Farms is the best drink...</td>\n",
       "      <td>5</td>\n",
       "      <td>The best drink mix</td>\n",
       "      <td>1332547200</td>\n",
       "      <td>03 24, 2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                  reviewerName helpful  \\\n",
       "0  A1ZQZ8RJS1XVTX  0657745316                      gsxrgirl  [0, 0]   \n",
       "1  A31W38VGZAUUM4  0700026444                      FIFA Lvr  [1, 1]   \n",
       "2  A3I0AV0UJX5OH0  1403796890                      Alicia b  [0, 0]   \n",
       "3  A3QAAOLIXKV383  1403796890  Danny K. Tilley \"Dan Tilley\"  [0, 0]   \n",
       "4   AB1A5EGHHVA9M  141278509X                       CHelmic  [1, 1]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  No sugar, no GMO garbage, no fillers that come...        5   \n",
       "1  This is my absolute, undisputed favorite tea r...        5   \n",
       "2  I ordered spongbob slippers and I got John Cen...        1   \n",
       "3  The cart is fine and works for the purpose for...        3   \n",
       "4  This product by Archer Farms is the best drink...        5   \n",
       "\n",
       "                      summary  unixReviewTime   reviewTime  \n",
       "0  Best vanilla I've ever had      1381449600  10 11, 2013  \n",
       "1               Terrific Tea!      1354752000   12 6, 2012  \n",
       "2                    grrrrrrr      1385942400   12 2, 2013  \n",
       "3      Storage on Wheels Cart      1307836800  06 12, 2011  \n",
       "4          The best drink mix      1332547200  03 24, 2012  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read-in the reviews\n",
    "reviews = pd.read_json(REVIEWS_PATH, lines=True)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop some of the columns\n",
    "reviews = reviews.drop(columns=['reviewerName', 'helpful', 'reviewTime', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1ZQZ8RJS1XVTX</td>\n",
       "      <td>0657745316</td>\n",
       "      <td>No sugar, no GMO garbage, no fillers that come...</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A31W38VGZAUUM4</td>\n",
       "      <td>0700026444</td>\n",
       "      <td>This is my absolute, undisputed favorite tea r...</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QAAOLIXKV383</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>The cart is fine and works for the purpose for...</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AB1A5EGHHVA9M</td>\n",
       "      <td>141278509X</td>\n",
       "      <td>This product by Archer Farms is the best drink...</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-03-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  \\\n",
       "0  A1ZQZ8RJS1XVTX  0657745316   \n",
       "1  A31W38VGZAUUM4  0700026444   \n",
       "2  A3I0AV0UJX5OH0  1403796890   \n",
       "3  A3QAAOLIXKV383  1403796890   \n",
       "4   AB1A5EGHHVA9M  141278509X   \n",
       "\n",
       "                                          reviewText  overall unixReviewTime  \n",
       "0  No sugar, no GMO garbage, no fillers that come...        5     2013-10-11  \n",
       "1  This is my absolute, undisputed favorite tea r...        5     2012-12-06  \n",
       "2  I ordered spongbob slippers and I got John Cen...        1     2013-12-02  \n",
       "3  The cart is fine and works for the purpose for...        3     2011-06-12  \n",
       "4  This product by Archer Farms is the best drink...        5     2012-03-24  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the utc timestamp to readable dates\n",
    "reviews['unixReviewTime'] = pd.to_datetime(reviews['unixReviewTime'],unit='s')\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Amazon dataset, our goal is to detect any potential harmful products by analyzing the user reviews and classifying them as presenting or not a possible health threat. Unfortunately, we lack the annotated data that would be needed to build a classifier.\n",
    "\n",
    "We will thus create our own annotated data by using **topic modelling** with the **Latent Dirichlet Allocation (LDA)** model. Our hope is that reviews of potential health-threatening products will be assigned to their own topic, topic that we would then be able to find by analyzing the words weights associated to that topic. Our labelling data pipeline is thus as follows :\n",
    "* We first start by sampling our reviews dataframe to work on a smaller set of reviews (for efficiency reasons).\n",
    "* We then preprocess the reviews : we remove any stopwords and stem the words with the help of **nltk** library to standardize them.\n",
    "* Using the **gensim** library, we create a corpus representing all stemmed reviews in a **bag of words** representation\n",
    "* Once we have our corpus, we create and train our LDA model to create our topics.\n",
    "* Now that we have our topics, we explore them to find health-related ones.\n",
    "* Finally, we assign each reviews to a topic by querying our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fares/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fares/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download stopwords and wordnet for stemming (only need to be executed once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128037</th>\n",
       "      <td>AF8OGMNOEAQ0K</td>\n",
       "      <td>B000EMQFBM</td>\n",
       "      <td>This product was extremely hard to find locall...</td>\n",
       "      <td>5</td>\n",
       "      <td>2009-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491755</th>\n",
       "      <td>A33BQJ9FBUONOI</td>\n",
       "      <td>B001H0FI22</td>\n",
       "      <td>I have the Nescafe Dolce Gusto machine in my o...</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470924</th>\n",
       "      <td>A26H17W5A8SALY</td>\n",
       "      <td>B001EQ5ERI</td>\n",
       "      <td>This makes a delicious, full-bodied, creamy cu...</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491263</th>\n",
       "      <td>A1V9OL87W94ZS1</td>\n",
       "      <td>B001H0FHXW</td>\n",
       "      <td>I really like my coffee maker and do not reall...</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836489</th>\n",
       "      <td>A3CIN9F0B9LFFQ</td>\n",
       "      <td>B004H1SORE</td>\n",
       "      <td>I handed these to my husband and found empty c...</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-01-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin  \\\n",
       "128037   AF8OGMNOEAQ0K  B000EMQFBM   \n",
       "491755  A33BQJ9FBUONOI  B001H0FI22   \n",
       "470924  A26H17W5A8SALY  B001EQ5ERI   \n",
       "491263  A1V9OL87W94ZS1  B001H0FHXW   \n",
       "836489  A3CIN9F0B9LFFQ  B004H1SORE   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "128037  This product was extremely hard to find locall...        5   \n",
       "491755  I have the Nescafe Dolce Gusto machine in my o...        5   \n",
       "470924  This makes a delicious, full-bodied, creamy cu...        5   \n",
       "491263  I really like my coffee maker and do not reall...        5   \n",
       "836489  I handed these to my husband and found empty c...        4   \n",
       "\n",
       "       unixReviewTime  \n",
       "128037     2009-11-30  \n",
       "491755     2011-01-25  \n",
       "470924     2013-05-03  \n",
       "491263     2012-12-29  \n",
       "836489     2014-01-12  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_label = reviews.sample(frac=0.1, replace=True, random_state=1)\n",
    "\n",
    "reviews_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to process the reviews using the nltk library :\n",
    "* We tokenize the sentence,\n",
    "* remove any potential stop words,\n",
    "* remove tokens containing only punctuations (such as '!!!', '...', etc.. which where quite common),\n",
    "* remove words below a given length,\n",
    "* stem the words to have them all represented in a standardized way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order', 'spongbob', 'slipper', 'got', 'john']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_text(sentence):\n",
    "    token_words = nltk.word_tokenize(sentence)\n",
    "    no_stopwords = [word.lower() for word in token_words if word not in stop_words and not \\\n",
    "                    all(c in string.punctuation for c in word) and not len(word) < 2]\n",
    "    return [stemmer.stem(word) for word in no_stopwords]\n",
    "\n",
    "print(process_text('I ordered spongbob slippers and I got John'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new column to our dataframe containing the processed reviewText (notice that we only consider reviews with a low score, under the fair assumption that reviews exposing health issues would have a low rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewStemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>925255</th>\n",
       "      <td>A1R9YASZP0VPXJ</td>\n",
       "      <td>B0052HX7Y2</td>\n",
       "      <td>Pho is one of my all-time favorite foods. When...</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>[pho, one, all-tim, favorit, food, when, saw, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575956</th>\n",
       "      <td>A2XKJ1KX6XUHYP</td>\n",
       "      <td>B002863BIW</td>\n",
       "      <td>I love Bragg's apple cider vinegar, the best t...</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-07-12</td>\n",
       "      <td>[love, bragg, 's, appl, cider, vinegar, best, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914041</th>\n",
       "      <td>AN9N29UZ7R6J2</td>\n",
       "      <td>B0050OJ9WQ</td>\n",
       "      <td>surprised , love Bacon , love chocalate , but ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-12-13</td>\n",
       "      <td>[surpris, love, bacon, love, chocal, done, lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130091</th>\n",
       "      <td>A19AR53KOQSWFT</td>\n",
       "      <td>B000EONEU0</td>\n",
       "      <td>My yogurt smelled like yogurt, but it was runn...</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>[my, yogurt, smell, like, yogurt, runni, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>A17Z1ZN1I68IVF</td>\n",
       "      <td>B00005C2M3</td>\n",
       "      <td>Don't waste your time with this seller.  didn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-04</td>\n",
       "      <td>[do, n't, wast, time, seller, n't, read, revie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin  \\\n",
       "925255  A1R9YASZP0VPXJ  B0052HX7Y2   \n",
       "575956  A2XKJ1KX6XUHYP  B002863BIW   \n",
       "914041   AN9N29UZ7R6J2  B0050OJ9WQ   \n",
       "130091  A19AR53KOQSWFT  B000EONEU0   \n",
       "1046    A17Z1ZN1I68IVF  B00005C2M3   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "925255  Pho is one of my all-time favorite foods. When...        2   \n",
       "575956  I love Bragg's apple cider vinegar, the best t...        1   \n",
       "914041  surprised , love Bacon , love chocalate , but ...        2   \n",
       "130091  My yogurt smelled like yogurt, but it was runn...        1   \n",
       "1046    Don't waste your time with this seller.  didn'...        1   \n",
       "\n",
       "       unixReviewTime                                      reviewStemmed  \n",
       "925255     2012-03-01  [pho, one, all-tim, favorit, food, when, saw, ...  \n",
       "575956     2014-07-12  [love, bragg, 's, appl, cider, vinegar, best, ...  \n",
       "914041     2013-12-13  [surpris, love, bacon, love, chocal, done, lik...  \n",
       "130091     2013-02-03  [my, yogurt, smell, like, yogurt, runni, like,...  \n",
       "1046       2014-04-04  [do, n't, wast, time, seller, n't, read, revie...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_label = reviews_label[reviews_label['overall'] < 3]\n",
    "reviews_label['reviewStemmed'] = reviews_label['reviewText'].apply(lambda x : process_text(x))\n",
    "\n",
    "reviews_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a dictionnary containing all the words found in our processed reviews, and our corpus consisting of all reviews in a bag of words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(reviews_label['reviewStemmed'])\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in reviews_label['reviewStemmed'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now instantiante and train our LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics = 15, passes=15, id2word=dictionary, minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at our found topics \n",
    "(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.033*\"price\" + 0.017*\"product\" + 0.014*\"\\'s\" + 0.011*\"get\" + 0.011*\"store\" + 0.010*\"per\" + 0.010*\"buy\" + 0.009*\"amazon\" + 0.009*\"the\" + 0.008*\"pay\"'),\n",
       " (1,\n",
       "  '0.052*\"tast\" + 0.033*\"like\" + 0.025*\"n\\'t\" + 0.014*\"flavor\" + 0.013*\"good\" + 0.013*\"one\" + 0.013*\"tri\" + 0.012*\"chocol\" + 0.012*\"would\" + 0.011*\"the\"'),\n",
       " (2,\n",
       "  '0.012*\"gummi\" + 0.012*\"bear\" + 0.009*\"salmon\" + 0.009*\"34\" + 0.008*\"candi\" + 0.008*\"eat\" + 0.007*\"bag\" + 0.007*\"the\" + 0.007*\"licoric\" + 0.006*\"product\"'),\n",
       " (3,\n",
       "  '0.024*\"order\" + 0.021*\"box\" + 0.017*\"the\" + 0.014*\"packag\" + 0.014*\"product\" + 0.013*\"receiv\" + 0.012*\"bag\" + 0.012*\"one\" + 0.011*\"n\\'t\" + 0.011*\"item\"'),\n",
       " (4,\n",
       "  '0.057*\"oil\" + 0.049*\"organ\" + 0.030*\"can\" + 0.017*\"coconut\" + 0.014*\"oliv\" + 0.014*\"dent\" + 0.013*\"honey\" + 0.012*\"soy\" + 0.012*\"use\" + 0.011*\"milk\"'),\n",
       " (5,\n",
       "  '0.072*\"coffe\" + 0.026*\"cup\" + 0.013*\"use\" + 0.013*\"tast\" + 0.013*\"n\\'t\" + 0.011*\"like\" + 0.010*\"k-cup\" + 0.010*\"the\" + 0.010*\"tri\" + 0.009*\"flavor\"'),\n",
       " (6,\n",
       "  '0.031*\"flavor\" + 0.017*\"like\" + 0.017*\"tast\" + 0.015*\"it\" + 0.015*\"cherri\" + 0.012*\"flour\" + 0.011*\"\\'s\" + 0.011*\"soda\" + 0.010*\"strawberri\" + 0.009*\"thi\"'),\n",
       " (7,\n",
       "  '0.033*\"n\\'t\" + 0.019*\"\\'s\" + 0.011*\"would\" + 0.010*\"product\" + 0.009*\"like\" + 0.008*\"buy\" + 0.008*\"bean\" + 0.008*\"formula\" + 0.007*\"tri\" + 0.007*\"\\'m\"'),\n",
       " (8,\n",
       "  '0.030*\"sugar\" + 0.021*\"ingredi\" + 0.019*\"product\" + 0.015*\"34\" + 0.011*\"sweet\" + 0.011*\"the\" + 0.010*\"natur\" + 0.009*\"\\'s\" + 0.009*\"corn\" + 0.009*\"list\"'),\n",
       " (9,\n",
       "  '0.021*\"n\\'t\" + 0.016*\"flavor\" + 0.014*\"like\" + 0.011*\"tast\" + 0.011*\"salt\" + 0.010*\"use\" + 0.010*\"\\'s\" + 0.010*\"product\" + 0.009*\"the\" + 0.008*\"it\"'),\n",
       " (10,\n",
       "  '0.054*\"tea\" + 0.021*\"tast\" + 0.021*\"n\\'t\" + 0.018*\"flavor\" + 0.017*\"like\" + 0.016*\"drink\" + 0.014*\"\\'s\" + 0.010*\"it\" + 0.009*\"tri\" + 0.008*\"green\"'),\n",
       " (11,\n",
       "  '0.027*\"product\" + 0.019*\"date\" + 0.019*\"amazon\" + 0.016*\"order\" + 0.016*\"expir\" + 0.012*\"purchas\" + 0.010*\"n\\'t\" + 0.009*\"time\" + 0.009*\"store\" + 0.009*\"month\"'),\n",
       " (12,\n",
       "  '0.017*\"n\\'t\" + 0.015*\"like\" + 0.013*\"it\" + 0.012*\"tast\" + 0.012*\"the\" + 0.009*\"tri\" + 0.009*\"eat\" + 0.008*\"\\'s\" + 0.008*\"soup\" + 0.008*\"thi\"'),\n",
       " (13,\n",
       "  '0.025*\"\\'s\" + 0.023*\"hot\" + 0.018*\"mix\" + 0.015*\"bottl\" + 0.013*\"it\" + 0.013*\"sauc\" + 0.012*\"tast\" + 0.011*\"syrup\" + 0.011*\"drink\" + 0.010*\"better\"'),\n",
       " (14,\n",
       "  '0.021*\"popcorn\" + 0.015*\"water\" + 0.012*\"coconut\" + 0.012*\"tast\" + 0.010*\"one\" + 0.010*\"pop\" + 0.010*\"order\" + 0.009*\"get\" + 0.008*\"n\\'t\" + 0.007*\"jerki\"')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our topics, we create a new column in our dataframe that tells us in which topic would that particular review be :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_label['topic'] = reviews_label['reviewStemmed'].apply( \\\n",
    "                        lambda x: sorted(ldamodel.get_document_topics(dictionary.doc2bow(x)), \\\n",
    "                                key=lambda x: (x[1]), reverse=True)[0][0])\n",
    "reviews_label.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's up next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
