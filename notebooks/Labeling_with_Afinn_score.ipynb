{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling the amazon reviews with Afinn sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(...)\n",
    "\n",
    "- Sentiment score from all of the words in Amazon reviews.\n",
    "- Sentiment score from only the most frequent words in Amazon reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment score from all of the words in Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "import gensim\n",
    "import collections\n",
    "import random\n",
    "from afinn import Afinn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is for making cleaned_reviews.json file. If you already have this, you don't need to run this cell.\n",
    "# # If you want to run, to remove all # at once, \"command + a\" -> \"command + /\".\n",
    "\n",
    "# #AMAZON DATA\n",
    "# REVIEWS_PATH = \"../data/02_processed/reviews_Grocery_and_Gourmet_Food.json.gz\"\n",
    "# META_PATH = \"../data/02_processed/meta_Grocery_and_Gourmet_Food.json.gz\"\n",
    "# #FDA DATA\n",
    "# path_press_released = \"/Users/tajimakeijiro/Desktop/amazon_project/Amazon-Reviews-Infectious-Disease/data/02_processed/FDA_press_released_2011_2019.csv\"\n",
    "# path_enforced = \"/Users/tajimakeijiro/Desktop/amazon_project/Amazon-Reviews-Infectious-Disease/data/02_processed/FDA_enforcements_2012-06_to_2019-10.csv\"\n",
    "\n",
    "# def sanitize(path, outpath):\n",
    "#     \"\"\"Converts a given compressed json to strict json and writes it in a new file\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     path : str\n",
    "#     The file location of the gzip-compressed json file\n",
    "#     outpath : str\n",
    "#     The path to the desired output file location \n",
    "\n",
    "#     \"\"\"\n",
    "#     g = gzip.open(path, 'r')\n",
    "\n",
    "#     out = open(outpath, 'w')\n",
    "\n",
    "#     for l in g:\n",
    "#         out.write(json.dumps(eval(l)) + '\\n')\n",
    "#     out.close()\n",
    "    \n",
    "# META_OUTPATH = \"cleaned_meta.json\"\n",
    "# REVIEWS_OUTPATH = \"cleaned_reviews.json\"\n",
    "\n",
    "# sanitize(META_PATH, META_OUTPATH)\n",
    "# sanitize(REVIEWS_PATH, REVIEWS_OUTPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tajimakeijiro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tajimakeijiro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download stopwords and wordnet for lemmatization (only need to be executed once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a json file made in the notebook \"Milestone2.ipynb\".\n",
    "REVIEWS_PATH = \"./cleaned_reviews.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A1ZQZ8RJS1XVTX</td>\n",
       "      <td>0657745316</td>\n",
       "      <td>No sugar, no GMO garbage, no fillers that come...</td>\n",
       "      <td>5</td>\n",
       "      <td>Best vanilla I've ever had</td>\n",
       "      <td>2013-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A31W38VGZAUUM4</td>\n",
       "      <td>0700026444</td>\n",
       "      <td>This is my absolute, undisputed favorite tea r...</td>\n",
       "      <td>5</td>\n",
       "      <td>Terrific Tea!</td>\n",
       "      <td>2012-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>grrrrrrr</td>\n",
       "      <td>2013-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A3QAAOLIXKV383</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>The cart is fine and works for the purpose for...</td>\n",
       "      <td>3</td>\n",
       "      <td>Storage on Wheels Cart</td>\n",
       "      <td>2011-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AB1A5EGHHVA9M</td>\n",
       "      <td>141278509X</td>\n",
       "      <td>This product by Archer Farms is the best drink...</td>\n",
       "      <td>5</td>\n",
       "      <td>The best drink mix</td>\n",
       "      <td>2012-03-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  \\\n",
       "0  A1ZQZ8RJS1XVTX  0657745316   \n",
       "1  A31W38VGZAUUM4  0700026444   \n",
       "2  A3I0AV0UJX5OH0  1403796890   \n",
       "3  A3QAAOLIXKV383  1403796890   \n",
       "4   AB1A5EGHHVA9M  141278509X   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  No sugar, no GMO garbage, no fillers that come...        5   \n",
       "1  This is my absolute, undisputed favorite tea r...        5   \n",
       "2  I ordered spongbob slippers and I got John Cen...        1   \n",
       "3  The cart is fine and works for the purpose for...        3   \n",
       "4  This product by Archer Farms is the best drink...        5   \n",
       "\n",
       "                      summary unixReviewTime  \n",
       "0  Best vanilla I've ever had     2013-10-11  \n",
       "1               Terrific Tea!     2012-12-06  \n",
       "2                    grrrrrrr     2013-12-02  \n",
       "3      Storage on Wheels Cart     2011-06-12  \n",
       "4          The best drink mix     2012-03-24  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_json(REVIEWS_PATH, lines=True)\n",
    "\n",
    "#TBD: Which columns to keep/remove\n",
    "reviews = reviews.drop(columns=['reviewerName', 'helpful', 'reviewTime'])\n",
    "\n",
    "#Convert the utc timestamp to readable dates\n",
    "reviews['unixReviewTime'] = pd.to_datetime(reviews['unixReviewTime'],unit='s')\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to process the reviews using the nltk library :\n",
    "* We tokenize the sentence,\n",
    "* remove any potential stop words,\n",
    "* remove tokens containing only punctuations (such as '!!!', '...', etc.. which where quite common),\n",
    "* remove words below a given length,\n",
    "* stem the words to have them all represented in a standardized way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ordered', 'spongbob', 'slippers', 'got', 'john']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Here we don't need stemming, because Afinn has same score for the before stemmed ones.\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "def process_text(sentence):\n",
    "    token_words = nltk.word_tokenize(sentence)\n",
    "    no_stopwords = [word.lower() for word in token_words if word not in stop_words and not \\\n",
    "                    all(c in string.punctuation for c in word) and not len(word) < 2]\n",
    "    # return [stemmer.stem(word) for word in no_stopwords]\n",
    "    return [(word) for word in no_stopwords]\n",
    "\n",
    "print(process_text('I ordered spongbob slippers and I got John'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new column to our dataframe containing the processed reviewText (notice that we only keep reviews with a low score, under the fair assumption that reviews exposing health issues would have a low rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewStemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>grrrrrrr</td>\n",
       "      <td>2013-12-02</td>\n",
       "      <td>[ordered, spongbob, slippers, got, john, cena,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>A3DTB6RVENLQ9Q</td>\n",
       "      <td>1453060375</td>\n",
       "      <td>Don't buy this item - rip off at this price.  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Oops.  Made a mistake and ordered this.  I mis...</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>[do, n't, buy, item, rip, price, my, bad, mist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>A3KJ9TZ2HLL7SA</td>\n",
       "      <td>5901002482</td>\n",
       "      <td>I wrote an earlier scathing review of this pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>Packaging problem</td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>[wrote, earlier, scathing, review, product, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>ACEL2LY99MAB0</td>\n",
       "      <td>6162362183</td>\n",
       "      <td>I read the reviews before I bought it. It got ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Very disappointed.</td>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>[read, reviews, bought, it, got, excited, revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>A2F3CK8F9VIFPL</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>I bought it because i like green tea but the t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yuck</td>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>[bought, like, green, tea, taste, bad, came, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin  \\\n",
       "2   A3I0AV0UJX5OH0  1403796890   \n",
       "5   A3DTB6RVENLQ9Q  1453060375   \n",
       "46  A3KJ9TZ2HLL7SA  5901002482   \n",
       "48   ACEL2LY99MAB0  6162362183   \n",
       "61  A2F3CK8F9VIFPL  616719923X   \n",
       "\n",
       "                                           reviewText  overall  \\\n",
       "2   I ordered spongbob slippers and I got John Cen...        1   \n",
       "5   Don't buy this item - rip off at this price.  ...        1   \n",
       "46  I wrote an earlier scathing review of this pro...        1   \n",
       "48  I read the reviews before I bought it. It got ...        2   \n",
       "61  I bought it because i like green tea but the t...        1   \n",
       "\n",
       "                                              summary unixReviewTime  \\\n",
       "2                                            grrrrrrr     2013-12-02   \n",
       "5   Oops.  Made a mistake and ordered this.  I mis...     2013-03-03   \n",
       "46                                  Packaging problem     2012-11-28   \n",
       "48                                 Very disappointed.     2014-04-21   \n",
       "61                                               Yuck     2013-07-29   \n",
       "\n",
       "                                        reviewStemmed  \n",
       "2   [ordered, spongbob, slippers, got, john, cena,...  \n",
       "5   [do, n't, buy, item, rip, price, my, bad, mist...  \n",
       "46  [wrote, earlier, scathing, review, product, wh...  \n",
       "48  [read, reviews, bought, it, got, excited, revi...  \n",
       "61  [bought, like, green, tea, taste, bad, came, m...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = reviews.copy()\n",
    "stemmed = stemmed[stemmed['overall'] < 3]\n",
    "stemmed['reviewStemmed'] = stemmed['reviewText'].apply(lambda x : process_text(x))\n",
    "\n",
    "stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we simply store the dataframe in a pickle for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed.to_pickle(\"reviews_tokened_tenth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "\n",
    "# This cell takes long\n",
    "stemmed['afinnWords'] = stemmed['reviewText'].apply(lambda x : afinn.find_all(x))\n",
    "stemmed['afinnScores'] = stemmed['reviewText'].apply(lambda x : afinn.scores_with_pattern(x))\n",
    "stemmed['afinnTotalScore'] = stemmed['reviewText'].apply(lambda x : afinn.score_with_pattern(x))\n",
    "stemmed['afinnWordsLen'] = stemmed['afinnWords'].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that flatten one column into a list\n",
    "def flatten(nested_list):\n",
    "    return [e for inner_list in nested_list for e in inner_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688\n"
     ]
    }
   ],
   "source": [
    "l_stemmed = stemmed['afinnWords'] .values.tolist()\n",
    "l_stemmed = flatten(l_stemmed)\n",
    "print(len(set(l_stemmed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed['afinnMeanScore'] = stemmed['afinnTotalScore']/stemmed['afinnWordsLen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewStemmed</th>\n",
       "      <th>afinnWords</th>\n",
       "      <th>afinnScores</th>\n",
       "      <th>afinnTotalScore</th>\n",
       "      <th>afinnWordsLen</th>\n",
       "      <th>afinnMeanScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>grrrrrrr</td>\n",
       "      <td>2013-12-02</td>\n",
       "      <td>[ordered, spongbob, slippers, got, john, cena,...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>A3DTB6RVENLQ9Q</td>\n",
       "      <td>1453060375</td>\n",
       "      <td>Don't buy this item - rip off at this price.  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Oops.  Made a mistake and ordered this.  I mis...</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>[do, n't, buy, item, rip, price, my, bad, mist...</td>\n",
       "      <td>[bad, mistake, pay, pay]</td>\n",
       "      <td>[-3, -2, -1, -1]</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>A3KJ9TZ2HLL7SA</td>\n",
       "      <td>5901002482</td>\n",
       "      <td>I wrote an earlier scathing review of this pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>Packaging problem</td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>[wrote, earlier, scathing, review, product, wh...</td>\n",
       "      <td>[harsh, apologize, disappointed, protect, hope...</td>\n",
       "      <td>[-2, -1, -2, 1, 2, -2]</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>ACEL2LY99MAB0</td>\n",
       "      <td>6162362183</td>\n",
       "      <td>I read the reviews before I bought it. It got ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Very disappointed.</td>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>[read, reviews, bought, it, got, excited, revi...</td>\n",
       "      <td>[excited, good, destroyed, mad, disappointed, ...</td>\n",
       "      <td>[3, 3, -3, -3, -2, 3]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>A2F3CK8F9VIFPL</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>I bought it because i like green tea but the t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yuck</td>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>[bought, like, green, tea, taste, bad, came, m...</td>\n",
       "      <td>[like, bad]</td>\n",
       "      <td>[2, -3]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin  \\\n",
       "2   A3I0AV0UJX5OH0  1403796890   \n",
       "5   A3DTB6RVENLQ9Q  1453060375   \n",
       "46  A3KJ9TZ2HLL7SA  5901002482   \n",
       "48   ACEL2LY99MAB0  6162362183   \n",
       "61  A2F3CK8F9VIFPL  616719923X   \n",
       "\n",
       "                                           reviewText  overall  \\\n",
       "2   I ordered spongbob slippers and I got John Cen...        1   \n",
       "5   Don't buy this item - rip off at this price.  ...        1   \n",
       "46  I wrote an earlier scathing review of this pro...        1   \n",
       "48  I read the reviews before I bought it. It got ...        2   \n",
       "61  I bought it because i like green tea but the t...        1   \n",
       "\n",
       "                                              summary unixReviewTime  \\\n",
       "2                                            grrrrrrr     2013-12-02   \n",
       "5   Oops.  Made a mistake and ordered this.  I mis...     2013-03-03   \n",
       "46                                  Packaging problem     2012-11-28   \n",
       "48                                 Very disappointed.     2014-04-21   \n",
       "61                                               Yuck     2013-07-29   \n",
       "\n",
       "                                        reviewStemmed  \\\n",
       "2   [ordered, spongbob, slippers, got, john, cena,...   \n",
       "5   [do, n't, buy, item, rip, price, my, bad, mist...   \n",
       "46  [wrote, earlier, scathing, review, product, wh...   \n",
       "48  [read, reviews, bought, it, got, excited, revi...   \n",
       "61  [bought, like, green, tea, taste, bad, came, m...   \n",
       "\n",
       "                                           afinnWords             afinnScores  \\\n",
       "2                                             [happy]                     [3]   \n",
       "5                            [bad, mistake, pay, pay]        [-3, -2, -1, -1]   \n",
       "46  [harsh, apologize, disappointed, protect, hope...  [-2, -1, -2, 1, 2, -2]   \n",
       "48  [excited, good, destroyed, mad, disappointed, ...   [3, 3, -3, -3, -2, 3]   \n",
       "61                                        [like, bad]                 [2, -3]   \n",
       "\n",
       "    afinnTotalScore  afinnWordsLen  afinnMeanScore  \n",
       "2               3.0              1        3.000000  \n",
       "5              -7.0              4       -1.750000  \n",
       "46             -4.0              6       -0.666667  \n",
       "48              1.0              6        0.166667  \n",
       "61             -1.0              2       -0.500000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_words = stemmed['afinnWords'].values.tolist()\n",
    "l_words = flatten(l_words)\n",
    "l_scores = [afinn.scores_with_pattern(x) for x in l_words]\n",
    "words_scores = list(zip(l_words, l_scores)) \n",
    "positive_words = set([word for word, score in words_scores if score[0] > 0])\n",
    "negative_words = set([word for word, score in words_scores if score[0] < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">overall</th>\n",
       "      <th colspan=\"5\" halign=\"left\">afinnTotalScore</th>\n",
       "      <th colspan=\"8\" halign=\"left\">afinnMeanScore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afinnWordsLen</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7287.0</td>\n",
       "      <td>1.346370</td>\n",
       "      <td>0.475846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7287.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19320.0</td>\n",
       "      <td>1.381832</td>\n",
       "      <td>0.485848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19320.0</td>\n",
       "      <td>0.389337</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19320.0</td>\n",
       "      <td>0.389337</td>\n",
       "      <td>2.176076</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>26116.0</td>\n",
       "      <td>1.384515</td>\n",
       "      <td>0.486490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26116.0</td>\n",
       "      <td>0.798399</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26116.0</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>1.569018</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>25653.0</td>\n",
       "      <td>1.381008</td>\n",
       "      <td>0.485644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25653.0</td>\n",
       "      <td>1.143219</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25653.0</td>\n",
       "      <td>0.381073</td>\n",
       "      <td>1.310980</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20578.0</td>\n",
       "      <td>1.375498</td>\n",
       "      <td>0.484263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20578.0</td>\n",
       "      <td>1.550296</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20578.0</td>\n",
       "      <td>0.387574</td>\n",
       "      <td>1.164667</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.007143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>160.5</td>\n",
       "      <td>192.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>0.627433</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.686620</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>1.130282</td>\n",
       "      <td>1.352113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.038889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.257426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>1.619433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               overall                                               \\\n",
       "                 count      mean       std  min  25%  50%  75%  max   \n",
       "afinnWordsLen                                                         \n",
       "0               7287.0  1.346370  0.475846  1.0  1.0  1.0  2.0  2.0   \n",
       "1              19320.0  1.381832  0.485848  1.0  1.0  1.0  2.0  2.0   \n",
       "2              26116.0  1.384515  0.486490  1.0  1.0  1.0  2.0  2.0   \n",
       "3              25653.0  1.381008  0.485644  1.0  1.0  1.0  2.0  2.0   \n",
       "4              20578.0  1.375498  0.484263  1.0  1.0  1.0  2.0  2.0   \n",
       "...                ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "140                1.0  2.000000       NaN  2.0  2.0  2.0  2.0  2.0   \n",
       "142                2.0  1.000000  0.000000  1.0  1.0  1.0  1.0  1.0   \n",
       "180                1.0  1.000000       NaN  1.0  1.0  1.0  1.0  1.0   \n",
       "202                1.0  1.000000       NaN  1.0  1.0  1.0  1.0  1.0   \n",
       "247                1.0  2.000000       NaN  2.0  2.0  2.0  2.0  2.0   \n",
       "\n",
       "              afinnTotalScore              ...               afinnMeanScore  \\\n",
       "                        count        mean  ...    75%    max          count   \n",
       "afinnWordsLen                              ...                                \n",
       "0                      7287.0    0.000000  ...    0.0    0.0            0.0   \n",
       "1                     19320.0    0.389337  ...    2.0    5.0        19320.0   \n",
       "2                     26116.0    0.798399  ...    4.0    9.0        26116.0   \n",
       "3                     25653.0    1.143219  ...    4.0   12.0        25653.0   \n",
       "4                     20578.0    1.550296  ...    5.0   14.0        20578.0   \n",
       "...                       ...         ...  ...    ...    ...            ...   \n",
       "140                       1.0   -1.000000  ...   -1.0   -1.0            1.0   \n",
       "142                       2.0  129.000000  ...  160.5  192.0            2.0   \n",
       "180                       1.0   -7.000000  ...   -7.0   -7.0            1.0   \n",
       "202                       1.0   52.000000  ...   52.0   52.0            1.0   \n",
       "247                       1.0  400.000000  ...  400.0  400.0            1.0   \n",
       "\n",
       "                                                                           \\\n",
       "                   mean       std       min       25%       50%       75%   \n",
       "afinnWordsLen                                                               \n",
       "0                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1              0.389337  2.176076 -4.000000 -2.000000  1.000000  2.000000   \n",
       "2              0.399200  1.569018 -4.000000 -0.500000  0.500000  2.000000   \n",
       "3              0.381073  1.310980 -3.333333 -0.666667  0.333333  1.333333   \n",
       "4              0.387574  1.164667 -3.250000 -0.500000  0.500000  1.250000   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "140           -0.007143       NaN -0.007143 -0.007143 -0.007143 -0.007143   \n",
       "142            0.908451  0.627433  0.464789  0.686620  0.908451  1.130282   \n",
       "180           -0.038889       NaN -0.038889 -0.038889 -0.038889 -0.038889   \n",
       "202            0.257426       NaN  0.257426  0.257426  0.257426  0.257426   \n",
       "247            1.619433       NaN  1.619433  1.619433  1.619433  1.619433   \n",
       "\n",
       "                         \n",
       "                    max  \n",
       "afinnWordsLen            \n",
       "0                   NaN  \n",
       "1              5.000000  \n",
       "2              4.500000  \n",
       "3              4.000000  \n",
       "4              3.500000  \n",
       "...                 ...  \n",
       "140           -0.007143  \n",
       "142            1.352113  \n",
       "180           -0.038889  \n",
       "202            0.257426  \n",
       "247            1.619433  \n",
       "\n",
       "[93 rows x 24 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed.groupby(by=['afinnWordsLen']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment score from only the most frequent words in Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested_list):\n",
    "    return [e for inner_list in nested_list for e in inner_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_stemmed = stemmed['reviewStemmed'].values.tolist()\n",
    "l_stemmed = flatten(l_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentWords = collections.Counter(l_stemmed).most_common()[0:50000]\n",
    "frequentWords = [word for word, count in frequentWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_frequent = stemmed.copy()\n",
    "stemmed_frequent['afinnWords'] = stemmed['afinnWords'].apply(lambda x : [i for i in x if i in frequentWords])\n",
    "stemmed_frequent['afinnScores'] = stemmed_frequent['reviewText'].apply(lambda x : afinn.scores_with_pattern(x))\n",
    "stemmed_frequent['afinnTotalScore'] = stemmed_frequent['reviewText'].apply(lambda x : afinn.score_with_pattern(x))\n",
    "stemmed_frequent['afinnWordsLen'] = stemmed_frequent['afinnWords'].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_frequent['afinnMeanScore'] = stemmed_frequent['afinnTotalScore']/stemmed_frequent['afinnWordsLen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewStemmed</th>\n",
       "      <th>afinnWords</th>\n",
       "      <th>afinnScores</th>\n",
       "      <th>afinnTotalScore</th>\n",
       "      <th>afinnWordsLen</th>\n",
       "      <th>afinnMeanScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>grrrrrrr</td>\n",
       "      <td>2013-12-02</td>\n",
       "      <td>[ordered, spongbob, slippers, got, john, cena,...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>A3DTB6RVENLQ9Q</td>\n",
       "      <td>1453060375</td>\n",
       "      <td>Don't buy this item - rip off at this price.  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Oops.  Made a mistake and ordered this.  I mis...</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>[do, n't, buy, item, rip, price, my, bad, mist...</td>\n",
       "      <td>[bad, mistake, pay, pay]</td>\n",
       "      <td>[-3, -2, -1, -1]</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>A3KJ9TZ2HLL7SA</td>\n",
       "      <td>5901002482</td>\n",
       "      <td>I wrote an earlier scathing review of this pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>Packaging problem</td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>[wrote, earlier, scathing, review, product, wh...</td>\n",
       "      <td>[harsh, apologize, disappointed, protect, hope...</td>\n",
       "      <td>[-2, -1, -2, 1, 2, -2]</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>ACEL2LY99MAB0</td>\n",
       "      <td>6162362183</td>\n",
       "      <td>I read the reviews before I bought it. It got ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Very disappointed.</td>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>[read, reviews, bought, it, got, excited, revi...</td>\n",
       "      <td>[excited, good, destroyed, mad, disappointed, ...</td>\n",
       "      <td>[3, 3, -3, -3, -2, 3]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>A2F3CK8F9VIFPL</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>I bought it because i like green tea but the t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yuck</td>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>[bought, like, green, tea, taste, bad, came, m...</td>\n",
       "      <td>[like, bad]</td>\n",
       "      <td>[2, -3]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin  \\\n",
       "2   A3I0AV0UJX5OH0  1403796890   \n",
       "5   A3DTB6RVENLQ9Q  1453060375   \n",
       "46  A3KJ9TZ2HLL7SA  5901002482   \n",
       "48   ACEL2LY99MAB0  6162362183   \n",
       "61  A2F3CK8F9VIFPL  616719923X   \n",
       "\n",
       "                                           reviewText  overall  \\\n",
       "2   I ordered spongbob slippers and I got John Cen...        1   \n",
       "5   Don't buy this item - rip off at this price.  ...        1   \n",
       "46  I wrote an earlier scathing review of this pro...        1   \n",
       "48  I read the reviews before I bought it. It got ...        2   \n",
       "61  I bought it because i like green tea but the t...        1   \n",
       "\n",
       "                                              summary unixReviewTime  \\\n",
       "2                                            grrrrrrr     2013-12-02   \n",
       "5   Oops.  Made a mistake and ordered this.  I mis...     2013-03-03   \n",
       "46                                  Packaging problem     2012-11-28   \n",
       "48                                 Very disappointed.     2014-04-21   \n",
       "61                                               Yuck     2013-07-29   \n",
       "\n",
       "                                        reviewStemmed  \\\n",
       "2   [ordered, spongbob, slippers, got, john, cena,...   \n",
       "5   [do, n't, buy, item, rip, price, my, bad, mist...   \n",
       "46  [wrote, earlier, scathing, review, product, wh...   \n",
       "48  [read, reviews, bought, it, got, excited, revi...   \n",
       "61  [bought, like, green, tea, taste, bad, came, m...   \n",
       "\n",
       "                                           afinnWords             afinnScores  \\\n",
       "2                                             [happy]                     [3]   \n",
       "5                            [bad, mistake, pay, pay]        [-3, -2, -1, -1]   \n",
       "46  [harsh, apologize, disappointed, protect, hope...  [-2, -1, -2, 1, 2, -2]   \n",
       "48  [excited, good, destroyed, mad, disappointed, ...   [3, 3, -3, -3, -2, 3]   \n",
       "61                                        [like, bad]                 [2, -3]   \n",
       "\n",
       "    afinnTotalScore  afinnWordsLen  afinnMeanScore  \n",
       "2               3.0              1        3.000000  \n",
       "5              -7.0              4       -1.750000  \n",
       "46             -4.0              6       -0.666667  \n",
       "48              1.0              6        0.166667  \n",
       "61             -1.0              2       -0.500000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_frequent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_frequent_words = stemmed_frequent['afinnWords'].values.tolist()\n",
    "l_frequent_words = flatten(l_frequent_words)\n",
    "l_frequent_scores = [afinn.scores_with_pattern(x) for x in l_frequent_words]\n",
    "words_scores = list(zip(l_frequent_words, l_frequent_scores)) \n",
    "positive_frequent_words = set([word for word, score in words_scores if score[0] > 0])\n",
    "negative_frequent_words = set([word for word, score in words_scores if score[0] < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">overall</th>\n",
       "      <th colspan=\"5\" halign=\"left\">afinnTotalScore</th>\n",
       "      <th colspan=\"8\" halign=\"left\">afinnMeanScore</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afinnWordsLen</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7287.0</td>\n",
       "      <td>1.346370</td>\n",
       "      <td>0.475846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7287.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19320.0</td>\n",
       "      <td>1.381832</td>\n",
       "      <td>0.485848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19320.0</td>\n",
       "      <td>0.389337</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19320.0</td>\n",
       "      <td>0.389337</td>\n",
       "      <td>2.176076</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>26116.0</td>\n",
       "      <td>1.384515</td>\n",
       "      <td>0.486490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26116.0</td>\n",
       "      <td>0.798399</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26116.0</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>1.569018</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>25653.0</td>\n",
       "      <td>1.381008</td>\n",
       "      <td>0.485644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25653.0</td>\n",
       "      <td>1.143219</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25653.0</td>\n",
       "      <td>0.381073</td>\n",
       "      <td>1.310980</td>\n",
       "      <td>-3.333333</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20578.0</td>\n",
       "      <td>1.375498</td>\n",
       "      <td>0.484263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20578.0</td>\n",
       "      <td>1.550296</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20578.0</td>\n",
       "      <td>0.387574</td>\n",
       "      <td>1.164667</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.007143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>160.5</td>\n",
       "      <td>192.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>0.627433</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.686620</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>1.130282</td>\n",
       "      <td>1.352113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>-0.038889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.257426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>1.619433</td>\n",
       "      <td>1.619433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               overall                                               \\\n",
       "                 count      mean       std  min  25%  50%  75%  max   \n",
       "afinnWordsLen                                                         \n",
       "0               7287.0  1.346370  0.475846  1.0  1.0  1.0  2.0  2.0   \n",
       "1              19320.0  1.381832  0.485848  1.0  1.0  1.0  2.0  2.0   \n",
       "2              26116.0  1.384515  0.486490  1.0  1.0  1.0  2.0  2.0   \n",
       "3              25653.0  1.381008  0.485644  1.0  1.0  1.0  2.0  2.0   \n",
       "4              20578.0  1.375498  0.484263  1.0  1.0  1.0  2.0  2.0   \n",
       "...                ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "140                1.0  2.000000       NaN  2.0  2.0  2.0  2.0  2.0   \n",
       "142                2.0  1.000000  0.000000  1.0  1.0  1.0  1.0  1.0   \n",
       "180                1.0  1.000000       NaN  1.0  1.0  1.0  1.0  1.0   \n",
       "202                1.0  1.000000       NaN  1.0  1.0  1.0  1.0  1.0   \n",
       "247                1.0  2.000000       NaN  2.0  2.0  2.0  2.0  2.0   \n",
       "\n",
       "              afinnTotalScore              ...               afinnMeanScore  \\\n",
       "                        count        mean  ...    75%    max          count   \n",
       "afinnWordsLen                              ...                                \n",
       "0                      7287.0    0.000000  ...    0.0    0.0            0.0   \n",
       "1                     19320.0    0.389337  ...    2.0    5.0        19320.0   \n",
       "2                     26116.0    0.798399  ...    4.0    9.0        26116.0   \n",
       "3                     25653.0    1.143219  ...    4.0   12.0        25653.0   \n",
       "4                     20578.0    1.550296  ...    5.0   14.0        20578.0   \n",
       "...                       ...         ...  ...    ...    ...            ...   \n",
       "140                       1.0   -1.000000  ...   -1.0   -1.0            1.0   \n",
       "142                       2.0  129.000000  ...  160.5  192.0            2.0   \n",
       "180                       1.0   -7.000000  ...   -7.0   -7.0            1.0   \n",
       "202                       1.0   52.000000  ...   52.0   52.0            1.0   \n",
       "247                       1.0  400.000000  ...  400.0  400.0            1.0   \n",
       "\n",
       "                                                                           \\\n",
       "                   mean       std       min       25%       50%       75%   \n",
       "afinnWordsLen                                                               \n",
       "0                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1              0.389337  2.176076 -4.000000 -2.000000  1.000000  2.000000   \n",
       "2              0.399200  1.569018 -4.000000 -0.500000  0.500000  2.000000   \n",
       "3              0.381073  1.310980 -3.333333 -0.666667  0.333333  1.333333   \n",
       "4              0.387574  1.164667 -3.250000 -0.500000  0.500000  1.250000   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "140           -0.007143       NaN -0.007143 -0.007143 -0.007143 -0.007143   \n",
       "142            0.908451  0.627433  0.464789  0.686620  0.908451  1.130282   \n",
       "180           -0.038889       NaN -0.038889 -0.038889 -0.038889 -0.038889   \n",
       "202            0.257426       NaN  0.257426  0.257426  0.257426  0.257426   \n",
       "247            1.619433       NaN  1.619433  1.619433  1.619433  1.619433   \n",
       "\n",
       "                         \n",
       "                    max  \n",
       "afinnWordsLen            \n",
       "0                   NaN  \n",
       "1              5.000000  \n",
       "2              4.500000  \n",
       "3              4.000000  \n",
       "4              3.500000  \n",
       "...                 ...  \n",
       "140           -0.007143  \n",
       "142            1.352113  \n",
       "180           -0.038889  \n",
       "202            0.257426  \n",
       "247            1.619433  \n",
       "\n",
       "[93 rows x 24 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed.groupby(by=['afinnWordsLen']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For UI, make dislike words lists.\n",
    "From Amazon reviews, we extracted the words which have minus score in Afinn as dislike words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bless', 'interests', 'careful', 'motivation', 'jewel']\n",
      "['denied', 'diseases', 'mad', 'deceit', 'suicides']\n"
     ]
    }
   ],
   "source": [
    "# Like and Dislike words list from all of the reviews.\n",
    "print(random.sample(positive_words, 5))\n",
    "print(random.sample(negative_words, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joke', 'vitality', 'justice', 'positively', 'compliment']\n",
      "['hates', 'disapproval', 'chaos', 'stabbed', 'disruption']\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(positive_frequent_words, 5))\n",
    "print(random.sample(negative_frequent_words, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([positive_words, negative_words, positive_frequent_words, negative_frequent_words], index = [\"Positive_words\", \"Negative_words\", \"Positive_words_only_frequent\", \"Negative_words_only_frequent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1694</th>\n",
       "      <th>1695</th>\n",
       "      <th>1696</th>\n",
       "      <th>1697</th>\n",
       "      <th>1698</th>\n",
       "      <th>1699</th>\n",
       "      <th>1700</th>\n",
       "      <th>1701</th>\n",
       "      <th>1702</th>\n",
       "      <th>1703</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Positive_words</td>\n",
       "      <td>defenders</td>\n",
       "      <td>admires</td>\n",
       "      <td>donates</td>\n",
       "      <td>comfortable</td>\n",
       "      <td>awesome</td>\n",
       "      <td>adequate</td>\n",
       "      <td>leading</td>\n",
       "      <td>endorses</td>\n",
       "      <td>fearless</td>\n",
       "      <td>immune</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Negative_words</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>sinful</td>\n",
       "      <td>undermines</td>\n",
       "      <td>offender</td>\n",
       "      <td>helpless</td>\n",
       "      <td>misused</td>\n",
       "      <td>exhausted</td>\n",
       "      <td>refused</td>\n",
       "      <td>infesting</td>\n",
       "      <td>apathy</td>\n",
       "      <td>...</td>\n",
       "      <td>obstructs</td>\n",
       "      <td>worsens</td>\n",
       "      <td>cried</td>\n",
       "      <td>protest</td>\n",
       "      <td>inoperative</td>\n",
       "      <td>fraudulence</td>\n",
       "      <td>worries</td>\n",
       "      <td>cruel</td>\n",
       "      <td>chastise</td>\n",
       "      <td>unethical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Positive_words_only_frequent</td>\n",
       "      <td>defenders</td>\n",
       "      <td>donates</td>\n",
       "      <td>comfortable</td>\n",
       "      <td>awesome</td>\n",
       "      <td>adequate</td>\n",
       "      <td>leading</td>\n",
       "      <td>endorses</td>\n",
       "      <td>immune</td>\n",
       "      <td>trusted</td>\n",
       "      <td>reach</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Negative_words_only_frequent</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>sinful</td>\n",
       "      <td>offender</td>\n",
       "      <td>helpless</td>\n",
       "      <td>exhausted</td>\n",
       "      <td>refused</td>\n",
       "      <td>infesting</td>\n",
       "      <td>apathy</td>\n",
       "      <td>brutally</td>\n",
       "      <td>atrocity</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 1704 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0        1            2            3     \\\n",
       "Positive_words                 defenders  admires      donates  comfortable   \n",
       "Negative_words                frustrated   sinful   undermines     offender   \n",
       "Positive_words_only_frequent   defenders  donates  comfortable      awesome   \n",
       "Negative_words_only_frequent  frustrated   sinful     offender     helpless   \n",
       "\n",
       "                                   4         5          6         7     \\\n",
       "Positive_words                  awesome  adequate    leading  endorses   \n",
       "Negative_words                 helpless   misused  exhausted   refused   \n",
       "Positive_words_only_frequent   adequate   leading   endorses    immune   \n",
       "Negative_words_only_frequent  exhausted   refused  infesting    apathy   \n",
       "\n",
       "                                   8         9     ...       1694     1695  \\\n",
       "Positive_words                 fearless    immune  ...       None     None   \n",
       "Negative_words                infesting    apathy  ...  obstructs  worsens   \n",
       "Positive_words_only_frequent    trusted     reach  ...       None     None   \n",
       "Negative_words_only_frequent   brutally  atrocity  ...       None     None   \n",
       "\n",
       "                               1696     1697         1698         1699  \\\n",
       "Positive_words                 None     None         None         None   \n",
       "Negative_words                cried  protest  inoperative  fraudulence   \n",
       "Positive_words_only_frequent   None     None         None         None   \n",
       "Negative_words_only_frequent   None     None         None         None   \n",
       "\n",
       "                                 1700   1701      1702       1703  \n",
       "Positive_words                   None   None      None       None  \n",
       "Negative_words                worries  cruel  chastise  unethical  \n",
       "Positive_words_only_frequent     None   None      None       None  \n",
       "Negative_words_only_frequent     None   None      None       None  \n",
       "\n",
       "[4 rows x 1704 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/02_processed/Like_and_dislike_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
