{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Amazon Reviews to detect any potential health issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Amazon dataset, we will try to detect any potential harmful products by analyzing the user reviews. To do so, we will use **topic modelling** with the **Latent Dirichlet Allocation (LDA)** model. Our hope is that reviews of potential harmful products will be assigned to their own topic, topic that we would be able to find by analyzing the words weights associated to that topic. Our pipeline is as follows :\n",
    "* We first start by importing our data and removing any non-useful columns.\n",
    "* We then preprocess the reviews : we remove any stopwords and stem the words with the help of **nltk** library to standardize them.\n",
    "* Using the **gensim** library, we create a corpus representing all stemmed reviews in a **bag of words** representation\n",
    "* Finally, we run the LDA model to create our topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fares/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fares/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download stopwords and wordnet for lemmatization (only need to be executed once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEWS_PATH = \"cleaned_tenth.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1ZQZ8RJS1XVTX</td>\n",
       "      <td>0657745316</td>\n",
       "      <td>2013-10-11</td>\n",
       "      <td>No sugar, no GMO garbage, no fillers that come...</td>\n",
       "      <td>5</td>\n",
       "      <td>Best vanilla I've ever had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A31W38VGZAUUM4</td>\n",
       "      <td>0700026444</td>\n",
       "      <td>2012-12-06</td>\n",
       "      <td>This is my absolute, undisputed favorite tea r...</td>\n",
       "      <td>5</td>\n",
       "      <td>Terrific Tea!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>2013-12-02</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>grrrrrrr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QAAOLIXKV383</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>2011-06-12</td>\n",
       "      <td>The cart is fine and works for the purpose for...</td>\n",
       "      <td>3</td>\n",
       "      <td>Storage on Wheels Cart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AB1A5EGHHVA9M</td>\n",
       "      <td>141278509X</td>\n",
       "      <td>2012-03-24</td>\n",
       "      <td>This product by Archer Farms is the best drink...</td>\n",
       "      <td>5</td>\n",
       "      <td>The best drink mix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin unixReviewTime  \\\n",
       "0  A1ZQZ8RJS1XVTX  0657745316     2013-10-11   \n",
       "1  A31W38VGZAUUM4  0700026444     2012-12-06   \n",
       "2  A3I0AV0UJX5OH0  1403796890     2013-12-02   \n",
       "3  A3QAAOLIXKV383  1403796890     2011-06-12   \n",
       "4   AB1A5EGHHVA9M  141278509X     2012-03-24   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  No sugar, no GMO garbage, no fillers that come...        5   \n",
       "1  This is my absolute, undisputed favorite tea r...        5   \n",
       "2  I ordered spongbob slippers and I got John Cen...        1   \n",
       "3  The cart is fine and works for the purpose for...        3   \n",
       "4  This product by Archer Farms is the best drink...        5   \n",
       "\n",
       "                      summary  \n",
       "0  Best vanilla I've ever had  \n",
       "1               Terrific Tea!  \n",
       "2                    grrrrrrr  \n",
       "3      Storage on Wheels Cart  \n",
       "4          The best drink mix  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_json(REVIEWS_PATH, lines=True)\n",
    "\n",
    "#TBD: Which columns to keep/remove\n",
    "reviews = reviews.drop(columns=['reviewerName', 'helpful', 'reviewTime'])\n",
    "\n",
    "#Convert the utc timestamp to readable dates\n",
    "reviews['unixReviewTime'] = pd.to_datetime(reviews['unixReviewTime'],unit='s')\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to process the reviews using the nltk library :\n",
    "* We tokenize the sentence,\n",
    "* remove any potential stop words,\n",
    "* remove tokens containing only punctuations (such as '!!!', '...', etc.. which where quite common),\n",
    "* remove words below a given length,\n",
    "* stem the words to have them all represented in a standardized way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['order', 'spongbob', 'slipper', 'got', 'john']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_text(sentence):\n",
    "    token_words = nltk.word_tokenize(sentence)\n",
    "    no_stopwords = [word.lower() for word in token_words if word not in stop_words and not \\\n",
    "                    all(c in string.punctuation for c in word) and not len(word) < 2]\n",
    "    return [stemmer.stem(word) for word in no_stopwords]\n",
    "\n",
    "print(process_text('I ordered spongbob slippers and I got John'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new column to our dataframe containing the processed reviewText (notice that we only keep reviews with a low score, under the fair assumption that reviews exposing health issues would have a low rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewStemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>2013-12-02</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>grrrrrrr</td>\n",
       "      <td>[order, spongbob, slipper, got, john, cena, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3DTB6RVENLQ9Q</td>\n",
       "      <td>1453060375</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>Don't buy this item - rip off at this price.  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Oops.  Made a mistake and ordered this.  I mis...</td>\n",
       "      <td>[do, n't, buy, item, rip, price, my, bad, mist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>A3KJ9TZ2HLL7SA</td>\n",
       "      <td>5901002482</td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>I wrote an earlier scathing review of this pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>Packaging problem</td>\n",
       "      <td>[wrote, earlier, scath, review, product, while...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ACEL2LY99MAB0</td>\n",
       "      <td>6162362183</td>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>I read the reviews before I bought it. It got ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Very disappointed.</td>\n",
       "      <td>[read, review, bought, it, got, excit, review,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>A2F3CK8F9VIFPL</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>I bought it because i like green tea but the t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yuck</td>\n",
       "      <td>[bought, like, green, tea, tast, bad, came, me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin unixReviewTime  \\\n",
       "2   A3I0AV0UJX5OH0  1403796890     2013-12-02   \n",
       "5   A3DTB6RVENLQ9Q  1453060375     2013-03-03   \n",
       "46  A3KJ9TZ2HLL7SA  5901002482     2012-11-28   \n",
       "48   ACEL2LY99MAB0  6162362183     2014-04-21   \n",
       "61  A2F3CK8F9VIFPL  616719923X     2013-07-29   \n",
       "\n",
       "                                           reviewText  overall  \\\n",
       "2   I ordered spongbob slippers and I got John Cen...        1   \n",
       "5   Don't buy this item - rip off at this price.  ...        1   \n",
       "46  I wrote an earlier scathing review of this pro...        1   \n",
       "48  I read the reviews before I bought it. It got ...        2   \n",
       "61  I bought it because i like green tea but the t...        1   \n",
       "\n",
       "                                              summary  \\\n",
       "2                                            grrrrrrr   \n",
       "5   Oops.  Made a mistake and ordered this.  I mis...   \n",
       "46                                  Packaging problem   \n",
       "48                                 Very disappointed.   \n",
       "61                                               Yuck   \n",
       "\n",
       "                                        reviewStemmed  \n",
       "2   [order, spongbob, slipper, got, john, cena, ha...  \n",
       "5   [do, n't, buy, item, rip, price, my, bad, mist...  \n",
       "46  [wrote, earlier, scath, review, product, while...  \n",
       "48  [read, review, bought, it, got, excit, review,...  \n",
       "61  [bought, like, green, tea, tast, bad, came, me...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = reviews.copy()\n",
    "stemmed = stemmed[stemmed['overall'] < 3]\n",
    "stemmed['reviewStemmed'] = stemmed['reviewText'].apply(lambda x : process_text(x))\n",
    "\n",
    "stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we simply store the dataframe in a pickle for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed.to_pickle(\"reviews_stemmed_tenth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a dictionnary containing all the words found in our processed reviews, and our corpus consisting of all reviews in a bag of words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for c in corpus[:1]:\\n    for word, freq in c:\\n        print(dictionary[word] + \": \" + str(freq))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(stemmed['reviewStemmed'])\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in stemmed['reviewStemmed'].values]\n",
    "'''for c in corpus[:1]:\n",
    "    for word, freq in c:\n",
    "        print(dictionary[word] + \": \" + str(freq))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our LDA model and have a look at the found topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.048*\"\\'s\" + 0.019*\"joe\" + 0.018*\"trader\" + 0.016*\"n\\'t\" + 0.012*\"one\" + 0.011*\"nut\" + 0.010*\"hazelnut\" + 0.009*\"like\" + 0.009*\"\\'re\" + 0.009*\"flavor\"'),\n",
       " (1,\n",
       "  '0.060*\"brand\" + 0.028*\"cocoa\" + 0.022*\"lime\" + 0.018*\"juic\" + 0.018*\"crust\" + 0.017*\"sour\" + 0.016*\"costco\" + 0.013*\"key\" + 0.011*\"planter\" + 0.010*\"fell\"'),\n",
       " (2,\n",
       "  '0.050*\"jerki\" + 0.037*\"lipton\" + 0.028*\"beef\" + 0.025*\"n\\'t\" + 0.012*\"look\" + 0.011*\"money\" + 0.011*\"turkey\" + 0.010*\"tri\" + 0.010*\"like\" + 0.010*\"thi\"'),\n",
       " (3,\n",
       "  '0.080*\"packag\" + 0.061*\"veri\" + 0.051*\"bag\" + 0.035*\"disappoint\" + 0.031*\"the\" + 0.026*\"oatmeal\" + 0.025*\"plastic\" + 0.016*\"away\" + 0.013*\"candi\" + 0.011*\"one\"'),\n",
       " (4,\n",
       "  '0.010*\"pistachio\" + 0.008*\"bad\" + 0.008*\"he\" + 0.008*\"\\'s\" + 0.007*\"it\" + 0.007*\"n\\'t\" + 0.007*\"even\" + 0.007*\"decemb\" + 0.007*\"2014\" + 0.006*\"ick\"'),\n",
       " (5,\n",
       "  '0.035*\"product\" + 0.032*\"cereal\" + 0.018*\"\\'s\" + 0.017*\"one\" + 0.014*\"food\" + 0.012*\"tast\" + 0.010*\"eat\" + 0.008*\"would\" + 0.008*\"it\" + 0.007*\"good\"'),\n",
       " (6,\n",
       "  '0.050*\"mix\" + 0.033*\"flour\" + 0.028*\"bread\" + 0.028*\"use\" + 0.026*\"rice\" + 0.024*\"bake\" + 0.022*\"gluten\" + 0.020*\"make\" + 0.019*\"free\" + 0.017*\"recip\"'),\n",
       " (7,\n",
       "  '0.081*\"date\" + 0.065*\"expir\" + 0.036*\"month\" + 0.016*\"store\" + 0.016*\"year\" + 0.015*\"amazon\" + 0.014*\"buy\" + 0.013*\"ago\" + 0.013*\"\\'s\" + 0.011*\"receiv\"'),\n",
       " (8,\n",
       "  '0.090*\"popcorn\" + 0.063*\"pop\" + 0.033*\"truffl\" + 0.027*\"dog\" + 0.018*\"kernel\" + 0.017*\"snack\" + 0.015*\"corn\" + 0.014*\"microwav\" + 0.014*\"bag\" + 0.014*\"n\\'t\"'),\n",
       " (9,\n",
       "  '0.021*\"order\" + 0.019*\"time\" + 0.015*\"lb\" + 0.014*\"deliveri\" + 0.012*\"three\" + 0.011*\"week\" + 0.010*\"day\" + 0.010*\"product\" + 0.010*\"still\" + 0.009*\"get\"'),\n",
       " (10,\n",
       "  '0.032*\"granola\" + 0.014*\"grit\" + 0.013*\"oat\" + 0.012*\"yogurt\" + 0.012*\"use\" + 0.010*\"air\" + 0.008*\"pearl\" + 0.008*\"thi\" + 0.008*\"product\" + 0.008*\"popper\"'),\n",
       " (11,\n",
       "  '0.016*\"cook\" + 0.013*\"\\'s\" + 0.013*\"bug\" + 0.011*\"soak\" + 0.009*\"n\\'t\" + 0.009*\"one\" + 0.008*\"go\" + 0.008*\"bag\" + 0.007*\"think\" + 0.006*\"littl\"'),\n",
       " (12,\n",
       "  '0.097*\"price\" + 0.021*\"buy\" + 0.018*\"thi\" + 0.014*\"product\" + 0.011*\"\\'s\" + 0.011*\"get\" + 0.010*\"amazon\" + 0.009*\"high\" + 0.009*\"the\" + 0.008*\"walmart\"'),\n",
       " (13,\n",
       "  '0.119*\"bean\" + 0.085*\"vanilla\" + 0.017*\"use\" + 0.012*\"smell\" + 0.012*\"the\" + 0.009*\"lover\" + 0.008*\"make\" + 0.008*\"thi\" + 0.007*\"buy\" + 0.007*\"purchas\"'),\n",
       " (14,\n",
       "  '0.087*\"can\" + 0.047*\"12\" + 0.033*\"amazon\" + 0.029*\"pack\" + 0.026*\"order\" + 0.023*\"case\" + 0.021*\"dent\" + 0.017*\"product\" + 0.014*\"packag\" + 0.013*\"subscrib\"'),\n",
       " (15,\n",
       "  '0.139*\"sugar\" + 0.033*\"corn\" + 0.031*\"ingredi\" + 0.028*\"syrup\" + 0.023*\"product\" + 0.017*\"free\" + 0.017*\"carb\" + 0.014*\"sweet\" + 0.013*\"high\" + 0.013*\"list\"'),\n",
       " (16,\n",
       "  '0.024*\"tast\" + 0.021*\"n\\'t\" + 0.015*\"cream\" + 0.014*\"would\" + 0.012*\"it\" + 0.012*\"good\" + 0.011*\"like\" + 0.011*\"tri\" + 0.011*\"bought\" + 0.009*\"\\'m\"'),\n",
       " (17,\n",
       "  '0.018*\"caus\" + 0.017*\"effect\" + 0.017*\"take\" + 0.016*\"day\" + 0.014*\"product\" + 0.013*\"work\" + 0.011*\"use\" + 0.011*\"read\" + 0.011*\"hour\" + 0.010*\"stop\"'),\n",
       " (18,\n",
       "  '0.045*\"n\\'t\" + 0.037*\"tast\" + 0.034*\"like\" + 0.027*\"flavor\" + 0.020*\"it\" + 0.019*\"would\" + 0.018*\"\\'s\" + 0.018*\"tri\" + 0.016*\"realli\" + 0.015*\"salti\"'),\n",
       " (19,\n",
       "  '0.030*\"milk\" + 0.025*\"formula\" + 0.025*\"babi\" + 0.019*\"soy\" + 0.016*\"product\" + 0.014*\"tri\" + 0.012*\"\\'s\" + 0.012*\"suppli\" + 0.011*\"n\\'t\" + 0.011*\"use\"'),\n",
       " (20,\n",
       "  '0.054*\"seed\" + 0.037*\"fat\" + 0.024*\"calori\" + 0.014*\"nutrit\" + 0.012*\"sprout\" + 0.010*\"serv\" + 0.010*\"food\" + 0.009*\"consum\" + 0.008*\"\\'s\" + 0.008*\"flax\"'),\n",
       " (21,\n",
       "  '0.041*\"oil\" + 0.039*\"rabbit\" + 0.032*\"oliv\" + 0.017*\"fresh\" + 0.016*\"n\\'t\" + 0.014*\"wild\" + 0.009*\"whole\" + 0.008*\"easter\" + 0.008*\"planet\" + 0.007*\"make\"'),\n",
       " (22,\n",
       "  '0.038*\"farm\" + 0.026*\"\\'s\" + 0.024*\"raisin\" + 0.016*\"barri\" + 0.014*\"currant\" + 0.012*\"product\" + 0.012*\"berri\" + 0.011*\"n\\'t\" + 0.011*\"black\" + 0.011*\"grape\"'),\n",
       " (23,\n",
       "  '0.019*\"the\" + 0.017*\"yellow\" + 0.015*\"one\" + 0.013*\"flavor\" + 0.012*\"product\" + 0.011*\"color\" + 0.011*\"natur\" + 0.010*\"\\'s\" + 0.009*\"say\" + 0.009*\"label\"'),\n",
       " (24,\n",
       "  '0.052*\"tast\" + 0.044*\"n\\'t\" + 0.039*\"like\" + 0.039*\"they\" + 0.034*\"money\" + 0.032*\"these\" + 0.029*\"wast\" + 0.021*\"would\" + 0.019*\"stale\" + 0.019*\"hard\"'),\n",
       " (25,\n",
       "  '0.016*\"non\" + 0.013*\"diseas\" + 0.011*\"pomegran\" + 0.009*\"fake\" + 0.009*\"bagel\" + 0.009*\"autolyz\" + 0.009*\"good\" + 0.008*\"particl\" + 0.008*\"articl\" + 0.007*\"print\"'),\n",
       " (26,\n",
       "  '0.182*\"34\" + 0.019*\"product\" + 0.019*\"new\" + 0.016*\"the\" + 0.012*\"origin\" + 0.011*\"chang\" + 0.009*\"old\" + 0.008*\"use\" + 0.008*\"kraft\" + 0.007*\"n\\'t\"'),\n",
       " (27,\n",
       "  '0.087*\"jar\" + 0.019*\"n\\'t\" + 0.019*\"lid\" + 0.018*\"fit\" + 0.016*\"muffin\" + 0.016*\"would\" + 0.014*\"\\'s\" + 0.013*\"glass\" + 0.012*\"star\" + 0.011*\"it\"'),\n",
       " (28,\n",
       "  '0.052*\"the\" + 0.046*\"not\" + 0.040*\"and\" + 0.033*\"is\" + 0.033*\"to\" + 0.031*\"it\" + 0.029*\"of\" + 0.029*\"wa\" + 0.025*\"thi\" + 0.022*\"for\"'),\n",
       " (29,\n",
       "  '0.015*\"product\" + 0.014*\"would\" + 0.014*\"low-carb\" + 0.013*\"the\" + 0.010*\"pud\" + 0.008*\"one\" + 0.008*\"mate\" + 0.008*\"box\" + 0.006*\"thi\" + 0.006*\"mix\"'),\n",
       " (30,\n",
       "  '0.039*\"red\" + 0.033*\"\\'s\" + 0.028*\"bob\" + 0.027*\"mill\" + 0.017*\"product\" + 0.016*\"chew\" + 0.015*\"the\" + 0.014*\"like\" + 0.014*\"n\\'t\" + 0.013*\"teeth\"'),\n",
       " (31,\n",
       "  '0.059*\"noodl\" + 0.034*\"ginger\" + 0.027*\"product\" + 0.021*\"version\" + 0.016*\"cashew\" + 0.015*\"ingredi\" + 0.014*\"soup\" + 0.014*\"chang\" + 0.014*\"like\" + 0.014*\"bigger\"'),\n",
       " (32,\n",
       "  '0.167*\"sauc\" + 0.111*\"hot\" + 0.031*\"vinegar\" + 0.027*\"spici\" + 0.021*\"bottl\" + 0.020*\"heat\" + 0.015*\"flavor\" + 0.012*\"it\" + 0.011*\"\\'s\" + 0.010*\"mild\"'),\n",
       " (33,\n",
       "  '0.062*\"n\\'t\" + 0.030*\"tast\" + 0.022*\"like\" + 0.019*\"tri\" + 0.014*\"eat\" + 0.014*\"ca\" + 0.013*\"one\" + 0.013*\"\\'m\" + 0.012*\"could\" + 0.012*\"realli\"'),\n",
       " (34,\n",
       "  '0.034*\"return\" + 0.033*\"open\" + 0.027*\"amazon\" + 0.027*\"product\" + 0.024*\"order\" + 0.024*\"n\\'t\" + 0.022*\"packag\" + 0.015*\"seller\" + 0.015*\"one\" + 0.013*\"the\"'),\n",
       " (35,\n",
       "  '0.047*\"gift\" + 0.033*\"item\" + 0.027*\"basket\" + 0.024*\"the\" + 0.019*\"receiv\" + 0.018*\"order\" + 0.015*\"n\\'t\" + 0.014*\"christma\" + 0.014*\"would\" + 0.014*\"look\"'),\n",
       " (36,\n",
       "  '0.042*\"ounc\" + 0.036*\"cracker\" + 0.023*\"n\\'t\" + 0.020*\"special\" + 0.012*\"flower\" + 0.011*\"14\" + 0.010*\"the\" + 0.009*\"ca\" + 0.008*\"go\" + 0.008*\"11\"'),\n",
       " (37,\n",
       "  '0.044*\"potato\" + 0.033*\"coat\" + 0.029*\"like\" + 0.020*\"taco\" + 0.020*\"miso\" + 0.020*\"thick\" + 0.016*\"product\" + 0.015*\"fri\" + 0.012*\"shell\" + 0.012*\"toffe\"'),\n",
       " (38,\n",
       "  '0.119*\"milk\" + 0.056*\"whole\" + 0.041*\"oz\" + 0.036*\"tuscan\" + 0.021*\"gallon\" + 0.011*\"parti\" + 0.010*\"subscript\" + 0.009*\"fl\" + 0.009*\"128\" + 0.007*\"hole\"'),\n",
       " (39,\n",
       "  '0.112*\"organ\" + 0.077*\"\\'s\" + 0.017*\"pouch\" + 0.012*\"like\" + 0.010*\"product\" + 0.010*\"\\'m\" + 0.010*\"buy\" + 0.008*\"natur\" + 0.008*\"it\" + 0.007*\"food\"'),\n",
       " (40,\n",
       "  '0.150*\"chees\" + 0.045*\"pasta\" + 0.038*\"mac\" + 0.027*\"easi\" + 0.024*\"box\" + 0.024*\"kraft\" + 0.015*\"powder\" + 0.015*\"make\" + 0.014*\"\\'s\" + 0.014*\"macaroni\"'),\n",
       " (41,\n",
       "  '0.019*\"fruit\" + 0.018*\"clump\" + 0.016*\"slice\" + 0.013*\"product\" + 0.012*\"reorder\" + 0.012*\"sourc\" + 0.011*\"answer\" + 0.010*\"directli\" + 0.010*\"the\" + 0.010*\"passion\"'),\n",
       " (42,\n",
       "  '0.085*\"pie\" + 0.066*\"appl\" + 0.062*\"pumpkin\" + 0.044*\"biscuit\" + 0.033*\"sand\" + 0.023*\"pecan\" + 0.017*\"spice\" + 0.011*\"mash\" + 0.010*\"libbi\" + 0.009*\"\\'s\"'),\n",
       " (43,\n",
       "  '0.034*\"msg\" + 0.028*\"gum\" + 0.027*\"food\" + 0.027*\"product\" + 0.026*\"ingredi\" + 0.023*\"contain\" + 0.020*\"acid\" + 0.016*\"china\" + 0.016*\"health\" + 0.014*\"made\"'),\n",
       " (44,\n",
       "  '0.025*\"crab\" + 0.024*\"pepper\" + 0.023*\"smoke\" + 0.019*\"salmon\" + 0.018*\"sauc\" + 0.017*\"meat\" + 0.014*\"hot\" + 0.014*\"the\" + 0.014*\"\\'s\" + 0.012*\"product\"'),\n",
       " (45,\n",
       "  '0.105*\"coffe\" + 0.026*\"cup\" + 0.026*\"pod\" + 0.016*\"roast\" + 0.016*\"use\" + 0.015*\"like\" + 0.015*\"tast\" + 0.014*\"flavor\" + 0.013*\"weak\" + 0.010*\"senseo\"'),\n",
       " (46,\n",
       "  '0.066*\"season\" + 0.062*\"lemon\" + 0.054*\"sodium\" + 0.025*\"low\" + 0.021*\"preserv\" + 0.020*\"color\" + 0.015*\"product\" + 0.013*\"swallow\" + 0.012*\"soy\" + 0.011*\"per\"'),\n",
       " (47,\n",
       "  '0.016*\"n\\'t\" + 0.015*\"\\'s\" + 0.012*\"use\" + 0.012*\"product\" + 0.009*\"find\" + 0.009*\"real\" + 0.009*\"quinoa\" + 0.008*\"get\" + 0.008*\"compani\" + 0.007*\"cheap\"'),\n",
       " (48,\n",
       "  '0.042*\"use\" + 0.029*\"sweeten\" + 0.027*\"splenda\" + 0.026*\"stevia\" + 0.022*\"artifici\" + 0.015*\"product\" + 0.010*\"sweet\" + 0.010*\"sucralos\" + 0.010*\"chemic\" + 0.007*\"like\"'),\n",
       " (49,\n",
       "  '0.009*\"one\" + 0.009*\"green\" + 0.009*\"imposs\" + 0.008*\"\\'s\" + 0.008*\"the\" + 0.007*\"packag\" + 0.007*\"product\" + 0.006*\"work\" + 0.006*\"tip\" + 0.006*\"advis\"'),\n",
       " (50,\n",
       "  '0.048*\"advertis\" + 0.037*\"protein\" + 0.029*\"grain\" + 0.022*\"chili\" + 0.019*\"energi\" + 0.018*\"soy\" + 0.018*\"fals\" + 0.015*\"hull\" + 0.015*\"lavend\" + 0.013*\"product\"'),\n",
       " (51,\n",
       "  '0.047*\"store\" + 0.032*\"price\" + 0.027*\"local\" + 0.023*\"buy\" + 0.022*\"groceri\" + 0.021*\"get\" + 0.018*\"size\" + 0.018*\"much\" + 0.017*\"cost\" + 0.016*\"product\"'),\n",
       " (52,\n",
       "  '0.059*\"order\" + 0.033*\"compani\" + 0.022*\"amazon\" + 0.022*\"product\" + 0.020*\"never\" + 0.017*\"ship\" + 0.016*\"would\" + 0.016*\"sent\" + 0.016*\"call\" + 0.015*\"custom\"'),\n",
       " (53,\n",
       "  '0.040*\"water\" + 0.036*\"tast\" + 0.031*\"coconut\" + 0.023*\"like\" + 0.020*\"drink\" + 0.019*\"\\'s\" + 0.017*\"n\\'t\" + 0.015*\"flavor\" + 0.015*\"tri\" + 0.011*\"it\"'),\n",
       " (54,\n",
       "  '0.078*\"gf\" + 0.018*\"funni\" + 0.012*\"chocol\" + 0.012*\"face\" + 0.010*\"the\" + 0.009*\"tin\" + 0.008*\"desper\" + 0.008*\"street\" + 0.007*\"fruitcak\" + 0.007*\"it\"'),\n",
       " (55,\n",
       "  '0.092*\"honey\" + 0.027*\"blue\" + 0.013*\"how\" + 0.012*\"grown\" + 0.010*\"jamaica\" + 0.008*\"graham\" + 0.008*\"order\" + 0.007*\"n\\'t\" + 0.007*\"toy\" + 0.007*\"biscotti\"'),\n",
       " (56,\n",
       "  '0.090*\"bar\" + 0.056*\"chocol\" + 0.025*\"tast\" + 0.018*\"flavor\" + 0.017*\"the\" + 0.015*\"like\" + 0.014*\"one\" + 0.011*\"box\" + 0.010*\"eat\" + 0.010*\"\\'s\"'),\n",
       " (57,\n",
       "  '0.030*\"\\'s\" + 0.014*\"clam\" + 0.012*\"n\\'t\" + 0.011*\"food\" + 0.010*\"product\" + 0.007*\"us\" + 0.007*\"gmo\" + 0.007*\"know\" + 0.007*\"princ\" + 0.007*\"see\"'),\n",
       " (58,\n",
       "  '0.085*\"fruit\" + 0.071*\"dress\" + 0.038*\"salad\" + 0.032*\"crunchi\" + 0.024*\"pineappl\" + 0.020*\"dust\" + 0.019*\"ranch\" + 0.016*\"dip\" + 0.014*\"dri\" + 0.011*\"bottl\"'),\n",
       " (59,\n",
       "  '0.032*\"\\'s\" + 0.017*\"n\\'t\" + 0.013*\"tri\" + 0.011*\"product\" + 0.008*\"review\" + 0.008*\"would\" + 0.007*\"one\" + 0.007*\"like\" + 0.007*\"even\" + 0.007*\"go\"'),\n",
       " (60,\n",
       "  '0.146*\"oil\" + 0.134*\"peanut\" + 0.117*\"butter\" + 0.026*\"natur\" + 0.025*\"buckwheat\" + 0.023*\"hydrogen\" + 0.022*\"palm\" + 0.016*\"contain\" + 0.015*\"ingredi\" + 0.014*\"fat\"'),\n",
       " (61,\n",
       "  '0.054*\"thai\" + 0.026*\"restaur\" + 0.023*\"pad\" + 0.022*\"curri\" + 0.016*\"asian\" + 0.014*\"cant\" + 0.011*\"sauc\" + 0.011*\"use\" + 0.011*\"time\" + 0.010*\"dish\"'),\n",
       " (62,\n",
       "  '0.022*\"order\" + 0.020*\"receiv\" + 0.020*\"pictur\" + 0.019*\"the\" + 0.019*\"product\" + 0.016*\"bottl\" + 0.014*\"show\" + 0.014*\"use\" + 0.013*\"descript\" + 0.012*\"item\"'),\n",
       " (63,\n",
       "  '0.039*\"soup\" + 0.023*\"noodl\" + 0.017*\"chicken\" + 0.016*\"n\\'t\" + 0.014*\"eat\" + 0.014*\"one\" + 0.011*\"tri\" + 0.010*\"the\" + 0.010*\"away\" + 0.009*\"like\"'),\n",
       " (64,\n",
       "  '0.035*\"flavor\" + 0.031*\"almond\" + 0.030*\"n\\'t\" + 0.020*\"healthi\" + 0.017*\"raw\" + 0.016*\"like\" + 0.014*\"tast\" + 0.013*\"the\" + 0.012*\"product\" + 0.012*\"banana\"'),\n",
       " (65,\n",
       "  '0.162*\"salt\" + 0.023*\"tuna\" + 0.016*\"solid\" + 0.016*\"pretzel\" + 0.014*\"sea\" + 0.012*\"product\" + 0.011*\"thi\" + 0.009*\"kosher\" + 0.009*\"crystal\" + 0.009*\"buy\"'),\n",
       " (66,\n",
       "  '0.036*\"yeast\" + 0.020*\"n\\'t\" + 0.015*\"work\" + 0.014*\"\\'s\" + 0.013*\"sleep\" + 0.008*\"help\" + 0.008*\"ingredi\" + 0.007*\"oh\" + 0.007*\"it\" + 0.006*\"supplement\"'),\n",
       " (67,\n",
       "  '0.101*\"tast\" + 0.063*\"like\" + 0.029*\"it\" + 0.024*\"n\\'t\" + 0.022*\"thi\" + 0.018*\"the\" + 0.017*\"smell\" + 0.016*\"tri\" + 0.015*\"buy\" + 0.014*\"ever\"'),\n",
       " (68,\n",
       "  '0.152*\"tea\" + 0.030*\"flavor\" + 0.026*\"tast\" + 0.021*\"green\" + 0.020*\"like\" + 0.016*\"bag\" + 0.014*\"n\\'t\" + 0.012*\"drink\" + 0.012*\"box\" + 0.011*\"the\"'),\n",
       " (69,\n",
       "  '0.043*\"cooki\" + 0.035*\"chocol\" + 0.028*\"chip\" + 0.013*\"like\" + 0.011*\"n\\'t\" + 0.011*\"one\" + 0.010*\"\\'s\" + 0.010*\"batch\" + 0.010*\"time\" + 0.009*\"\\'ve\"'),\n",
       " (70,\n",
       "  '0.108*\"box\" + 0.034*\"cooki\" + 0.033*\"the\" + 0.024*\"packag\" + 0.021*\"arriv\" + 0.021*\"broken\" + 0.020*\"order\" + 0.018*\"open\" + 0.018*\"insid\" + 0.016*\"candi\"'),\n",
       " (71,\n",
       "  '0.020*\"sardin\" + 0.018*\"new\" + 0.017*\"plant\" + 0.014*\"the\" + 0.013*\"back\" + 0.013*\"\\'s\" + 0.010*\"dead\" + 0.010*\"n\\'t\" + 0.010*\"old\" + 0.009*\"use\"'),\n",
       " (72,\n",
       "  '0.042*\"product\" + 0.030*\"purchas\" + 0.019*\"amazon\" + 0.018*\"the\" + 0.017*\"order\" + 0.016*\"time\" + 0.012*\"box\" + 0.011*\"one\" + 0.010*\"review\" + 0.009*\"good\"'),\n",
       " (73,\n",
       "  '0.038*\"cherri\" + 0.013*\"\\'s\" + 0.011*\"egg\" + 0.010*\"it\" + 0.010*\"product\" + 0.009*\"soda\" + 0.008*\"peppercorn\" + 0.007*\"juic\" + 0.006*\"n\\'t\" + 0.005*\"cold\"'),\n",
       " (74,\n",
       "  '0.029*\"\\'s\" + 0.028*\"bowl\" + 0.023*\"anni\" + 0.019*\"chun\" + 0.012*\"n\\'t\" + 0.011*\"one\" + 0.010*\"ketchup\" + 0.009*\"sour\" + 0.008*\"get\" + 0.007*\"soup\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics = 75, passes=15, id2word=dictionary, minimum_probability=0)\n",
    "\n",
    "ldamodel.print_topics(num_topics=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.save('lda_tenth.model')\n",
    "#load : \n",
    "# ldamodel =  gensim.models.LdaModel.load('lda_tenth.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our topics, we create a new column in our dataframe that tells us in which topic would that particular review be :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewStemmed</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3I0AV0UJX5OH0</td>\n",
       "      <td>1403796890</td>\n",
       "      <td>2013-12-02</td>\n",
       "      <td>I ordered spongbob slippers and I got John Cen...</td>\n",
       "      <td>1</td>\n",
       "      <td>grrrrrrr</td>\n",
       "      <td>[order, spongbob, slipper, got, john, cena, ha...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3DTB6RVENLQ9Q</td>\n",
       "      <td>1453060375</td>\n",
       "      <td>2013-03-03</td>\n",
       "      <td>Don't buy this item - rip off at this price.  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Oops.  Made a mistake and ordered this.  I mis...</td>\n",
       "      <td>[do, n't, buy, item, rip, price, my, bad, mist...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>A3KJ9TZ2HLL7SA</td>\n",
       "      <td>5901002482</td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>I wrote an earlier scathing review of this pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>Packaging problem</td>\n",
       "      <td>[wrote, earlier, scath, review, product, while...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ACEL2LY99MAB0</td>\n",
       "      <td>6162362183</td>\n",
       "      <td>2014-04-21</td>\n",
       "      <td>I read the reviews before I bought it. It got ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Very disappointed.</td>\n",
       "      <td>[read, review, bought, it, got, excit, review,...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>A2F3CK8F9VIFPL</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>I bought it because i like green tea but the t...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yuck</td>\n",
       "      <td>[bought, like, green, tea, tast, bad, came, me...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin unixReviewTime  \\\n",
       "2   A3I0AV0UJX5OH0  1403796890     2013-12-02   \n",
       "5   A3DTB6RVENLQ9Q  1453060375     2013-03-03   \n",
       "46  A3KJ9TZ2HLL7SA  5901002482     2012-11-28   \n",
       "48   ACEL2LY99MAB0  6162362183     2014-04-21   \n",
       "61  A2F3CK8F9VIFPL  616719923X     2013-07-29   \n",
       "\n",
       "                                           reviewText  overall  \\\n",
       "2   I ordered spongbob slippers and I got John Cen...        1   \n",
       "5   Don't buy this item - rip off at this price.  ...        1   \n",
       "46  I wrote an earlier scathing review of this pro...        1   \n",
       "48  I read the reviews before I bought it. It got ...        2   \n",
       "61  I bought it because i like green tea but the t...        1   \n",
       "\n",
       "                                              summary  \\\n",
       "2                                            grrrrrrr   \n",
       "5   Oops.  Made a mistake and ordered this.  I mis...   \n",
       "46                                  Packaging problem   \n",
       "48                                 Very disappointed.   \n",
       "61                                               Yuck   \n",
       "\n",
       "                                        reviewStemmed  topic  \n",
       "2   [order, spongbob, slipper, got, john, cena, ha...     10  \n",
       "5   [do, n't, buy, item, rip, price, my, bad, mist...     12  \n",
       "46  [wrote, earlier, scath, review, product, while...     36  \n",
       "48  [read, review, bought, it, got, excit, review,...     16  \n",
       "61  [bought, like, green, tea, tast, bad, came, me...     68  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed['topic'] = stemmed['reviewStemmed'].apply( \\\n",
    "                        lambda x: sorted(ldamodel.get_document_topics(dictionary.doc2bow(x)), \\\n",
    "                                key=lambda x: (x[1]), reverse=True)[0][0])\n",
    "stemmed.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the topic 43 corresponds to our search..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We were looking for food coloring that is natural, NOT artificial, due to concerns about health issues caused by artificial coloring (ADHD, cancer, thyroid issues, etc.). These contain Red 40 and other baddies :( We will not be using them.\n",
      "The main ingredient in this is monosodium glutamate. If it is for miso soup, keep looking.\n",
      "I guess someone in the warehouse needed a pack of gum because when it arrive the box was ripped an a pack of gum removed.  Your very welcome.\n"
     ]
    }
   ],
   "source": [
    "healthReviews = stemmed[stemmed['topic'] == 43].reviewText.values \n",
    "\n",
    "print(healthReviews[0])\n",
    "print(healthReviews[1])\n",
    "print(healthReviews[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now visualise our topics : for that we make use of the **pyLDAvis** library, which makes it easy to create interactive topic modeling plots :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a14f3de0a546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlda_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlda_display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary=ldamodel.id2word, sort_topics=False)\n",
    "lda_display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
